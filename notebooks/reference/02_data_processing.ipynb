{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.use_inf_as_na = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba import NumbaDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"The default value of regex will change from True to False in a future version.\",\n",
    "                        category=FutureWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, get_package_path,\n",
    "    display_as_tabs, string_cleaning, merge_info, initialize_environment,\n",
    "    list_datasets, load_dataset, save_dataset\n",
    ")\n",
    "import ta_lib.eda.api as eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/raw/housing',\n",
      " '/cleaned/housing',\n",
      " '/train/housing/features',\n",
      " '/train/housing/target',\n",
      " '/test/housing/features',\n",
      " '/test/housing/target',\n",
      " '/score/housing/output']\n"
     ]
    }
   ],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)\n",
    "pprint(list_datasets(context))\n",
    "\n",
    "housing_df = load_dataset(context, 'raw/housing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and consolidation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clean individual tables "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning housing dataset:\n",
    "\n",
    "- Remove duplicate rows\n",
    "- Convert data types (e.g., ensure numeric types for price, area, etc.)\n",
    "- Handle missing values appropriately\n",
    "- Generate additional features like age_of_house, price_per_sqft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value', 'ocean_proximity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_clean = (\n",
    "    housing_df\n",
    "    .copy()\n",
    "    .drop_duplicates()\n",
    "    .replace({'': np.NaN})\n",
    "    .convert_dtypes()\n",
    ")\n",
    "\n",
    "# Generate derived features\n",
    "housing_df_clean['rooms_per_household'] = housing_df_clean['total_rooms'] / housing_df_clean['households']\n",
    "housing_df_clean['bedrooms_per_room'] = housing_df_clean['total_bedrooms'] / housing_df_clean['total_rooms']\n",
    "housing_df_clean['population_per_household'] = housing_df_clean['population'] / housing_df_clean['households']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "It's always a good idea to save cleaned tabular data using a storage format that supports the following \n",
    "\n",
    "1. preserves the type information\n",
    "2. language agnostic storage format\n",
    "3. Supports compression\n",
    "4. Supports customizing storage to optimize different data access patterns\n",
    "\n",
    "For larger datasets, the last two points become crucial.\n",
    "\n",
    "`Parquet` is one such file format that is very popular for storing tabular data. It has some nice properties:\n",
    "- Similar to pickles & RDS datasets, but compatible with all languages\n",
    "- Preserves the datatypes\n",
    "- Compresses the data and reduces the filesize\n",
    "- Good library support in Python and other languages\n",
    "- As a columnar storage we can efficiently read fewer columns\n",
    "- It also supports chunking data by groups of columns (for instance, by dates or a particular value of a key column) that makes loading subsets of the data fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, housing_df_clean, 'cleaned/housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, orders_df_clean, 'cleaned/orders')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Train, Validation and Test datasets\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We split the data into train, test (optionally, also a validation dataset)\n",
    "- In this example, we are binning the target into 10 quantiles and then use a Stratified Shuffle to split the data.\n",
    "- See sklearn documentation on the various available splitters\n",
    "- https://scikit-learn.org/stable/modules/classes.html#splitter-classes\n",
    "- This will go into production code (training only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = train_test_split(housing_df_clean, test_size=0.2, random_state=context.random_seed)\n",
    "housing_train, housing_test = splitter\n",
    "\n",
    "target_col = \"median_house_value\"\n",
    "\n",
    "train_X = housing_train.drop(columns=[target_col])\n",
    "train_y = housing_train[target_col]\n",
    "save_dataset(context, train_X, 'train/housing/features')\n",
    "save_dataset(context, train_y, 'train/housing/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = housing_test.drop(columns=[target_col])\n",
    "test_y = housing_test[target_col]\n",
    "save_dataset(context, test_X, 'test/housing/features')\n",
    "save_dataset(context, test_y, 'test/housing/target')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-lib-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
